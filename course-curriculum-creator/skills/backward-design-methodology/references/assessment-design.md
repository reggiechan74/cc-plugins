# Assessment Design for Intensive Workshops

Comprehensive guidance on designing assessments for 1-2 day workshops following backward design principles.

## Assessment Types for Different Cognitive Levels

### Remember Level Assessments
- Quick knowledge checks (5-10 questions)
- Matching exercises (terms to definitions)
- Short-answer identification
- Pre-workshop baseline tests

**Use sparingly in intensive workshops—foundation knowledge typically prerequisite.**

### Understand Level Assessments
- Explain concepts in own words
- Compare and contrast frameworks
- Summarize case studies
- Classify examples into categories

**Good for Day 1 morning check-ins.**

### Apply Level Assessments
- Use tools/frameworks on new scenarios
- Execute procedures with guidance
- Solve problems using learned methods
- Follow step-by-step processes

**Core assessment type for 1-day workshops.**

### Analyze Level Assessments
- Break down complex situations into components
- Identify patterns and relationships
- Distinguish relevant from irrelevant information
- Determine underlying structures

**Suitable for Day 1 afternoon or Day 2.**

### Evaluate Level Assessments
- Critique solutions using criteria
- Rank options with justification
- Make decisions based on evidence
- Assess quality against standards

**Advanced assessment for Day 2 of 2-day workshops.**

### Create Level Assessments
- Design original solutions
- Develop plans or strategies
- Generate new approaches
- Synthesize multiple concepts

**Capstone assessment for 2-day workshops only.**

## Performance Task Design

### Authentic Assessment Characteristics

**1. Real-world context**
- Situation students will encounter in their work
- Industry-specific scenarios and constraints
- Actual tools and resources they'll use

**2. Ill-structured problems**
- No single correct answer
- Requires judgment and decision-making
- Multiple valid approaches possible

**3. Sustained engagement**
- 30-90 minutes for meaningful tasks
- Multiple steps or phases
- Opportunity to revise and improve

**4. Observable products**
- Tangible deliverables (documents, presentations, models)
- Demonstrated processes (facilitation, analysis, execution)
- Measurable outcomes (solutions, recommendations, plans)

### Performance Task Template

```markdown
## Performance Task: [Descriptive Title]

**Learning Outcome(s) Addressed:**
- [Outcome 1]
- [Outcome 2]

**Scenario:**
[Realistic situation that provides context]

**Your Role:**
[What role students play—consultant, manager, analyst, etc.]

**Challenge:**
[The problem or opportunity to address]

**Task:**
[What students must produce or demonstrate]

**Resources Provided:**
- [Dataset, case study, tools, templates]

**Constraints:**
- Time: [Duration]
- Format: [Deliverable specifications]
- Audience: [Who will receive/review the output]

**Success Criteria:**
[How work will be evaluated—reference rubric]
```

### Example: PropTech Workshop Performance Task

```markdown
## Performance Task: PropTech Solution Evaluation

**Learning Outcomes Addressed:**
- Analyze PropTech use cases using established frameworks
- Evaluate PropTech solutions for specific real estate challenges

**Scenario:**
You are a consultant for a mid-sized commercial real estate firm exploring PropTech adoption. The firm manages 2M sq ft of office space across 15 buildings and struggles with inefficient property maintenance and tenant communication.

**Your Role:**
Senior technology advisor tasked with recommending PropTech solutions

**Challenge:**
Three PropTech vendors have submitted proposals. Your firm can only invest in one solution this year. Senior leadership needs your analysis and recommendation.

**Task:**
1. Analyze the firm's challenge using value chain analysis
2. Evaluate each vendor solution against defined criteria
3. Recommend one solution with justification
4. Outline implementation roadmap (90 days)

**Resources Provided:**
- Vendor proposals (3 documents)
- Evaluation rubric template
- Company background brief

**Constraints:**
- Time: 90 minutes
- Format: 2-page recommendation memo + 15-minute presentation
- Audience: CFO and VP of Operations

**Success Criteria:**
See attached analytical rubric (criteria: analysis depth, evaluation rigor, recommendation clarity, implementation feasibility)
```

## Rubric Development

### Rubric Structure

**Analytical rubrics** (recommended for intensive workshops):
- Multiple criteria evaluated separately
- Each criterion has 1-5 scale with descriptors
- Allows specific feedback on strengths/weaknesses

**Format:**
```
| Criterion | 1 (Beginning) | 2 (Developing) | 3 (Proficient) | 4 (Advanced) | 5 (Exemplary) |
|-----------|---------------|----------------|----------------|--------------|---------------|
| [Criterion 1] | [Descriptor] | [Descriptor] | [Descriptor] | [Descriptor] | [Descriptor] |
```

### Criterion Selection

Choose 3-6 criteria per rubric that directly map to learning outcomes.

**Example criteria for different outcome types:**

**Analysis outcomes:**
- Depth of analysis
- Use of appropriate frameworks
- Identification of key factors
- Evidence support

**Evaluation outcomes:**
- Clarity of criteria
- Thoroughness of comparison
- Strength of justification
- Quality of recommendation

**Creation outcomes:**
- Feasibility of solution
- Creativity and originality
- Completeness of plan
- Alignment with constraints

### Descriptor Writing

Write descriptors that clearly differentiate performance levels.

**Poor descriptors (vague):**
- Level 3: "Good analysis"
- Level 4: "Very good analysis"
- Level 5: "Excellent analysis"

**Strong descriptors (specific):**
- Level 3: "Applies framework correctly, identifies 3-4 key factors, limited depth of explanation"
- Level 4: "Applies framework systematically, identifies 5-6 key factors, explains relationships between factors"
- Level 5: "Applies framework expertly, identifies all relevant factors, explains complex interactions and implications"

**Descriptor development tips:**
- Use observable behaviors and concrete evidence
- Specify quantity where applicable (number of factors, sources, examples)
- Describe quality differences (correct vs. systematic vs. expert application)
- Include both what's present and what's absent at each level

### Complete Rubric Example

```markdown
## Rubric: PropTech Solution Evaluation

| Criterion | 1 (Beginning) | 2 (Developing) | 3 (Proficient) | 4 (Advanced) | 5 (Exemplary) |
|-----------|---------------|----------------|----------------|--------------|---------------|
| **Problem Analysis** | No framework used; superficial description of challenge | Framework partially applied; 1-2 factors identified | Value chain framework correctly applied; 3-4 factors identified with explanation | Systematic framework application; 5-6 factors identified with relationships explained | Expert framework application; comprehensive factor identification with complex interactions and implications analyzed |
| **Solution Evaluation** | No criteria specified; impressionistic comparison | Basic criteria listed; incomplete vendor comparison | Clear criteria defined; each vendor evaluated against all criteria | Criteria well-justified; thorough vendor comparison with evidence | Sophisticated multi-dimensional criteria; rigorous vendor analysis with weighted scoring and sensitivity analysis |
| **Recommendation Clarity** | No clear recommendation or lacks justification | Recommendation made but weakly supported | Clear recommendation with 2-3 supporting reasons | Strong recommendation with compelling rationale addressing multiple stakeholder perspectives | Exceptionally clear recommendation with comprehensive justification, risk mitigation, and alternative scenarios addressed |
| **Implementation Feasibility** | No implementation plan or unrealistic timeline | Generic implementation steps; timeline disconnected from reality | Logical 90-day plan with major milestones; realistic timeline | Detailed implementation roadmap with dependencies, resources, and contingencies | Sophisticated implementation strategy with phased approach, risk management, success metrics, and governance structure |
```

## Embedded Assessment Strategies

### Formative Assessment Techniques

**Purpose:** Check understanding during instruction to adjust teaching and provide feedback

**1. Exit Tickets** (2-3 minutes, end of module)
- Prompt: "Write one key insight from this module and one remaining question"
- Instructor reviews during break to address gaps in next module

**2. Think-Pair-Share** (5-10 minutes, mid-module)
- Individual thinking (1-2 min)
- Pair discussion (3-4 min)
- Class share-out (2-3 min)
- Reveals misconceptions for immediate correction

**3. Quick Polls** (1-2 minutes, after concept introduction)
- Multiple choice or Likert scale questions
- Use tools: Poll Everywhere, Mentimeter, or hand-raising
- Gauge comprehension before moving forward

**4. One-Minute Papers** (5 minutes, transition points)
- Prompt: "Summarize the key concept we just covered in 2-3 sentences"
- Quick skim reveals who's following and who's lost

**5. Peer Review** (10-15 minutes, during practice activities)
- Students evaluate each other's work using simplified rubric
- Provides feedback while reinforcing quality standards
- Instructor circulates to monitor peer feedback quality

**6. Misconception Check** (3-5 minutes, after common stumbling points)
- Present common errors and ask students to identify the mistake
- Discuss why error is tempting and how to avoid
- Preventive assessment technique

### Integration into Lesson Plans

**Module structure with embedded assessment:**

```
Module X: [Topic] (90 minutes)

Introduction (10 min)
- Quick poll: "Rate your familiarity with [topic]" (1 min) ← Formative
- Overview and learning objective preview (9 min)

Instruction (20 min)
- Concept explanation with examples (18 min)
- Think-pair-share: "How would you apply this to your work?" (2 min) ← Formative

Guided Practice (25 min)
- Structured exercise with instructor support (22 min)
- Exit ticket: "One insight, one question" (3 min) ← Formative

Independent Practice (30 min)
- Performance task (authentic application) (30 min) ← Summative

Debrief (5 min)
- Address common questions from exit tickets (3 min)
- Preview next module connection (2 min)
```

## Assessment Alignment Validation

### Alignment Matrix

Use this matrix to validate outcome-assessment alignment:

| Learning Outcome | Cognitive Level | Assessment Type | Assessment Timing | Success Criteria | Rubric Criterion |
|------------------|-----------------|-----------------|-------------------|------------------|------------------|
| [Outcome text] | [Bloom's level] | [Performance task, quiz, etc.] | [Module X, Day X] | [What demonstrates mastery] | [Which rubric criterion measures this] |

**Validation questions:**
1. Does every outcome have at least one assessment?
2. Does assessment method match cognitive level?
3. Are assessments distributed across workshop timeline?
4. Do success criteria clearly define mastery?
5. Do rubric criteria align with outcomes?

### Cognitive Level Matching

Ensure assessment method matches outcome cognitive level:

| Cognitive Level | Appropriate Assessment Methods | Inappropriate Assessment Methods |
|-----------------|--------------------------------|----------------------------------|
| Remember | Multiple choice, matching, labeling | Essays, performance tasks |
| Understand | Explanation, summary, comparison | Rote recall, pure application |
| Apply | Procedure execution, problem-solving | Definition, memorization |
| Analyze | Case study analysis, pattern identification | Simple application, recall |
| Evaluate | Critique, ranking with justification, decision-making | Analysis without judgment |
| Create | Design, plan development, synthesis | Evaluation using others' criteria |

**Misalignment examples:**

❌ **Poor alignment:**
- Outcome: "Evaluate PropTech solutions using defined criteria" (Evaluate level)
- Assessment: Multiple-choice quiz on PropTech categories (Remember level)

✅ **Strong alignment:**
- Outcome: "Evaluate PropTech solutions using defined criteria" (Evaluate level)
- Assessment: Rank three solutions with justification using rubric (Evaluate level)

## Workshop-Specific Assessment Considerations

### Time Constraints

**1-day workshops:**
- Maximum 2-3 summative assessments
- Each assessment: 30-45 minutes
- Frequent formative checks (every 60-90 minutes)
- Final capstone: 45-60 minutes

**2-day workshops:**
- Maximum 4-5 summative assessments
- Day 1 capstone: 45-60 minutes
- Day 2 capstone: 60-90 minutes
- Formative checks throughout

### Practicality Filters

**Ask for each assessment:**
1. Can it be completed in available time?
2. Can it be evaluated quickly (during workshop or immediately after)?
3. Does it provide immediate value to students?
4. Is it logistically feasible (materials, technology, space)?
5. Can students apply the feedback immediately?

**If answer is "no" to any question, redesign the assessment.**

### Feedback Turnaround

**During workshop:**
- Formative feedback: Immediate or within 5-10 minutes
- Performance task feedback: During activity (circulate) or at debrief (5-10 minutes)
- Peer feedback: Built into activity time

**Post-workshop:**
- Final capstone evaluation: Within 3-5 business days
- Detailed rubric-based feedback with scores
- Personalized recommendations for continued learning

## Pre/Post Assessment Strategy

### Pre-Workshop Assessment

**Purpose:** Establish baseline, customize workshop pacing, identify student needs

**Format:**
- 10-15 minute self-assessment survey
- Sent 1 week before workshop
- Mix of confidence ratings and knowledge checks

**Example pre-assessment:**

```markdown
## Pre-Workshop Assessment: PropTech Fundamentals

**Background:**
1. Years of experience in real estate: [Scale 0-5, 5-10, 10-15, 15+]
2. Exposure to PropTech: [None, Minimal, Moderate, Extensive]

**Knowledge Check:**
3. Rate your understanding: "PropTech is..." [1-5 scale]
4. Can you name 3 PropTech categories? [Yes/No/Maybe]
5. Have you evaluated PropTech solutions before? [Yes/No]

**Learning Goals:**
6. What specific skill do you want to develop? [Open text]
7. What challenge do you hope to solve? [Open text]

**Preferences:**
8. Preferred learning mode: [Lecture-heavy / Balanced / Lab-heavy]
```

**Use pre-assessment data to:**
- Adjust examples to match audience experience
- Skip or accelerate foundation content if group is advanced
- Customize case studies to participant industries
- Form heterogeneous groups for peer learning

### Post-Workshop Assessment

**Purpose:** Measure learning gains, evaluate workshop effectiveness, identify gaps

**Format:**
- 10-15 minute survey
- Administered at workshop end or 24 hours post
- Mix of outcome-based questions and feedback

**Example post-assessment:**

```markdown
## Post-Workshop Assessment: PropTech Fundamentals

**Learning Outcomes:**
1. Rate your ability to analyze PropTech use cases using frameworks [1-5 scale]
2. Rate your confidence evaluating PropTech solutions [1-5 scale]
3. Rate your readiness to create implementation roadmaps [1-5 scale]

**Application:**
4. Will you apply skills from this workshop in next 30 days? [Yes/No/Maybe]
5. What will you apply first? [Open text]
6. What obstacles might prevent application? [Open text]

**Workshop Evaluation:**
7. Most valuable module: [List options]
8. Least valuable module: [List options]
9. Pacing: [Too slow / Just right / Too fast]
10. Activity balance: [Too much lecture / Balanced / Too much practice]

**Feedback:**
11. What should be added? [Open text]
12. What should be removed? [Open text]
13. Would you recommend to colleague? [Yes/No/Maybe + why]
```

**Use post-assessment data to:**
- Measure learning gains (compare pre/post ratings)
- Identify curriculum refinements (add/remove/adjust)
- Validate backward design alignment
- Iterate for next workshop offering

## Common Assessment Mistakes

### Mistake 1: Assessment-Activity Mismatch

❌ **Problem:**
- Outcome: Evaluate solutions
- Practice: Instructor demonstrates evaluation
- Assessment: Students evaluate solutions independently

**Why problematic:** Students never practiced the skill before assessment.

✅ **Solution:**
- Outcome: Evaluate solutions
- Practice: Students practice evaluation with feedback
- Assessment: Students evaluate solutions independently

### Mistake 2: Unrealistic Rubrics

❌ **Problem:**
Rubric has 8 criteria, each with detailed 6-level descriptors. Impossible to use in workshop timeframe.

✅ **Solution:**
Rubric has 4 criteria, each with clear 5-level descriptors. Can be applied quickly during activity.

### Mistake 3: Delayed Feedback

❌ **Problem:**
Students complete performance task. Instructor grades after workshop. Students never see feedback or learn from mistakes.

✅ **Solution:**
Students complete performance task. Peer review with rubric provides immediate feedback. Instructor circulates to validate. Debrief addresses common issues.

### Mistake 4: Trivial Assessments

❌ **Problem:**
- Outcome: Create PropTech implementation roadmaps (Create level)
- Assessment: Define "implementation roadmap" (Remember level)

**Why problematic:** Assessment doesn't measure the outcome.

✅ **Solution:**
- Outcome: Create PropTech implementation roadmaps (Create level)
- Assessment: Develop 90-day implementation plan for selected solution (Create level)

### Mistake 5: No Formative Assessment

❌ **Problem:**
120-minute module with instruction and practice, but no check-ins. Instructor discovers at end that students are confused.

✅ **Solution:**
120-minute module with instruction, guided practice (with check-in), independent practice. Instructor catches confusion early and adjusts.

---

Apply these assessment design principles to ensure evaluations measure intended outcomes, provide actionable feedback, and fit within intensive workshop timeframes. Align assessment methods with cognitive levels, embed formative checks throughout, and validate alignment using the provided matrices.
